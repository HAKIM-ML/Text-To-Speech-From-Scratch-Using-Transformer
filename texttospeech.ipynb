{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import pydub\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchaudio.functional import spectrogram\n",
    "from torchaudio.transforms import Spectrogram, MelScale, InverseMelScale, GriffinLim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hyperparams:\n",
    "  seed = 42\n",
    "\n",
    "  csv_path = \"LJSpeech-1.1/metadata.csv\"\n",
    "  wav_path = \"LJSpeech-1.1/wavs\"\n",
    "  save_path = \"params\"  \n",
    "  log_path = \"train_logs\"\n",
    "  \n",
    "  save_name = \"SimpleTransfromerTTS.pt\"\n",
    "\n",
    "  # Text transformations params\n",
    "  symbols = [\n",
    "    'EOS', ' ', '!', ',', '-', '.', \\\n",
    "    ';', '?', 'a', 'b', 'c', 'd', 'e', 'f', \\\n",
    "    'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', \\\n",
    "    'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'à', \\\n",
    "    'â', 'è', 'é', 'ê', 'ü', '’', '“', '”' \\\n",
    "  ]\n",
    "  \n",
    "  # Sounds transformations params\n",
    "  sr = 22050\n",
    "  n_fft = 2048\n",
    "  n_stft = int((n_fft//2) + 1)\n",
    "  \n",
    "  frame_shift = 0.0125 # seconds\n",
    "  hop_length = int(n_fft/8.0)\n",
    "  \n",
    "  frame_length = 0.05 # seconds  \n",
    "  win_length = int(n_fft/2.0)\n",
    "  \n",
    "  mel_freq = 128\n",
    "  max_mel_time = 1024\n",
    "  \n",
    "  max_db = 100  \n",
    "  scale_db = 10\n",
    "  ref = 4.0\n",
    "  power = 2.0\n",
    "  norm_db = 10 \n",
    "  ampl_multiplier = 10.0\n",
    "  ampl_amin = 1e-10\n",
    "  db_multiplier = 1.0\n",
    "  ampl_ref = 1.0\n",
    "  ampl_power = 1.0\n",
    "\n",
    "  # Model params\n",
    "  text_num_embeddings = 2*len(symbols)  \n",
    "  embedding_size = 256\n",
    "  encoder_embedding_size = 512 \n",
    "\n",
    "  dim_feedforward = 1024\n",
    "  postnet_embedding_size = 1024\n",
    "\n",
    "  encoder_kernel_size = 3\n",
    "  postnet_kernel_size = 5\n",
    "\n",
    "  # Other\n",
    "  batch_size = 32\n",
    "  grad_clip = 1.0\n",
    "  lr = 2.0 * 1e-4\n",
    "  r_gate = 1.0\n",
    "\n",
    "  step_print = 1000\n",
    "  step_test = 8000\n",
    "  step_save = 8000\n",
    "  min_label_db = -100\n",
    "hp = Hyperparams()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text To Sequence Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "symbol_to_id = {\n",
    "    s: i for i,s in enumerate(hp.symbols)\n",
    "}\n",
    "\n",
    "def text_to_seq(text):\n",
    "    text = text.lower()\n",
    "    seq = []\n",
    "    for s in text:\n",
    "        _id = symbol_to_id.get(s, None)\n",
    "        if id is not None:\n",
    "            seq.append(_id)\n",
    "            \n",
    "    seq.append(symbol_to_id['EOS'])\n",
    "    \n",
    "    return torch.IntTensor(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([15, 12, 19, 19, 22,  3,  1, 30, 22, 25, 19, 11,  0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(text_to_seq(\"Hello, World\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mask For Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_from_seq_length(\n",
    "    sequence_lengths: torch.Tensor,\n",
    "    max_length: int\n",
    ") ->torch.BoolTensor:\n",
    "    \"\"\"\n",
    "    our input was `[2, 2, 3]`, with a `max_length` of 4, we'd return\n",
    "    `[[1, 1, 0, 0], [1, 1, 0, 0], [1, 1, 1, 0]]`.\n",
    "    \"\"\"\n",
    "    # (batch size , max_length)\n",
    "    ones = sequence_lengths.new_ones(sequence_lengths.size(0), max_length)\n",
    "    range_tensor = ones.cumsum(dim = 1)\n",
    "    return sequence_lengths.unsqueeze(1) >= range_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mel Spectrogram Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.functional import spectrogram\n",
    "import torchaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchaudio\n",
    "from torchaudio.functional import spectrogram\n",
    "\n",
    "\n",
    "spec_transform = torchaudio.transforms.Spectrogram(\n",
    "    n_fft=hp.n_fft, \n",
    "    win_length=hp.win_length,\n",
    "    hop_length=hp.hop_length,\n",
    "    power=hp.power\n",
    ")\n",
    "\n",
    "\n",
    "mel_scale_transform = torchaudio.transforms.MelScale(\n",
    "  n_mels=hp.mel_freq, \n",
    "  sample_rate=hp.sr, \n",
    "  n_stft=hp.n_stft\n",
    ")\n",
    "\n",
    "\n",
    "mel_inverse_transform = torchaudio.transforms.InverseMelScale(\n",
    "  n_mels=hp.mel_freq, \n",
    "  sample_rate=hp.sr, \n",
    "  n_stft=hp.n_stft\n",
    ").cuda()\n",
    "\n",
    "\n",
    "griffnlim_transform = torchaudio.transforms.GriffinLim(\n",
    "    n_fft=hp.n_fft,\n",
    "    win_length=hp.win_length,\n",
    "    hop_length=hp.hop_length\n",
    ").cuda()\n",
    "\n",
    "\n",
    "def norm_mel_spec_db(mel_spec):  \n",
    "  mel_spec = ((2.0*mel_spec - hp.min_level_db) / (hp.max_db/hp.norm_db)) - 1.0\n",
    "  mel_spec = torch.clip(mel_spec, -hp.ref*hp.norm_db, hp.ref*hp.norm_db)\n",
    "  return mel_spec\n",
    "\n",
    "\n",
    "def denorm_mel_spec_db(mel_spec):\n",
    "  mel_spec = (((1.0 + mel_spec) * (hp.max_db/hp.norm_db)) + hp.min_level_db) / 2.0 \n",
    "  return mel_spec\n",
    "\n",
    "\n",
    "def pow_to_db_mel_spec(mel_spec):\n",
    "  mel_spec = torchaudio.functional.amplitude_to_DB(\n",
    "    mel_spec,\n",
    "    multiplier = hp.ampl_multiplier, \n",
    "    amin = hp.ampl_amin, \n",
    "    db_multiplier = hp.db_multiplier, \n",
    "    top_db = hp.max_db\n",
    "  )\n",
    "  mel_spec = mel_spec/hp.scale_db\n",
    "  return mel_spec\n",
    "\n",
    "\n",
    "def db_to_power_mel_spec(mel_spec):\n",
    "  mel_spec = mel_spec*hp.scale_db\n",
    "  mel_spec = torchaudio.functional.DB_to_amplitude(\n",
    "    mel_spec,\n",
    "    ref=hp.ampl_ref,\n",
    "    power=hp.ampl_power\n",
    "  )  \n",
    "  return mel_spec\n",
    "\n",
    "\n",
    "def convert_to_mel_spec(wav):\n",
    "  spec = spec_transform(wav)\n",
    "  mel_spec = mel_scale_transform(spec)\n",
    "  db_mel_spec = pow_to_db_mel_spec(mel_spec)\n",
    "  db_mel_spec = db_mel_spec.squeeze(0)\n",
    "  return db_mel_spec\n",
    "\n",
    "\n",
    "def inverse_mel_spec_to_wav(mel_spec):\n",
    "  power_mel_spec = db_to_power_mel_spec(mel_spec)\n",
    "  spectrogram = mel_inverse_transform(power_mel_spec)\n",
    "  pseudo_wav = griffnlim_transform(spectrogram)\n",
    "  return pseudo_wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mel_spec: torch.Size([128, 832])\n",
      "pseudo_wav: torch.Size([212736])\n"
     ]
    }
   ],
   "source": [
    "wav_path = \"LJSpeech-1.1/wavs/LJ001-0001.wav\" \n",
    "waveform, sample_rate = torchaudio.load(wav_path, normalize=True)\n",
    "mel_spec = convert_to_mel_spec(waveform)\n",
    "print(\"mel_spec:\", mel_spec.shape)\n",
    "pseudo_wav = inverse_mel_spec_to_wav(mel_spec.cuda())\n",
    "print(\"pseudo_wav:\", pseudo_wav.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset And DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextMelDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.cache = {}\n",
    "\n",
    "    def get_item(self, row):\n",
    "      wav_id = row[\"wav\"]                  \n",
    "      wav_path = f\"{hp.wav_path}/{wav_id}.wav\"\n",
    "\n",
    "      text = row[\"text_norm\"]\n",
    "      text = text_to_seq(text)\n",
    "\n",
    "      waveform, sample_rate = torchaudio.load(wav_path, normalize=True)\n",
    "      assert sample_rate == hp.sr\n",
    "\n",
    "      mel = convert_to_mel_spec(waveform)\n",
    "\n",
    "      return (text, mel)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "      row = self.df.iloc[index]\n",
    "      wav_id = row[\"wav\"]\n",
    "\n",
    "      text_mel = self.cache.get(wav_id)\n",
    "\n",
    "      if text_mel is None:\n",
    "        text_mel = self.get_item(row)\n",
    "        self.cache[wav_id] = text_mel\n",
    "      \n",
    "      return text_mel\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "\n",
    "def text_mel_collate_fn(batch):\n",
    "  text_length_max = torch.tensor(\n",
    "    [text.shape[-1] for text, _ in batch], \n",
    "    dtype=torch.int32\n",
    "  ).max()\n",
    "\n",
    "  mel_length_max = torch.tensor(\n",
    "    [mel.shape[-1] for _, mel in batch],\n",
    "    dtype=torch.int32\n",
    "  ).max()\n",
    "\n",
    "  \n",
    "  text_lengths = []\n",
    "  mel_lengths = []\n",
    "  texts_padded = []\n",
    "  mels_padded = []\n",
    "\n",
    "  for text, mel in batch:\n",
    "    text_length = text.shape[-1]      \n",
    "\n",
    "    text_padded = torch.nn.functional.pad(\n",
    "      text,\n",
    "      pad=[0, text_length_max-text_length],\n",
    "      value=0\n",
    "    )\n",
    "\n",
    "    mel_length = mel.shape[-1]\n",
    "    mel_padded = torch.nn.functional.pad(\n",
    "        mel,\n",
    "        pad=[0, mel_length_max-mel_length],\n",
    "        value=0\n",
    "    )\n",
    "\n",
    "    text_lengths.append(text_length)    \n",
    "    mel_lengths.append(mel_length)    \n",
    "    texts_padded.append(text_padded)    \n",
    "    mels_padded.append(mel_padded)\n",
    "\n",
    "  text_lengths = torch.tensor(text_lengths, dtype=torch.int32)\n",
    "  mel_lengths = torch.tensor(mel_lengths, dtype=torch.int32)\n",
    "  texts_padded = torch.stack(texts_padded, 0)\n",
    "  mels_padded = torch.stack(mels_padded, 0).transpose(1, 2)\n",
    "\n",
    "  stop_token_padded = mask_from_seq_length(\n",
    "      mel_lengths,\n",
    "      mel_length_max\n",
    "  )\n",
    "  stop_token_padded = (~stop_token_padded).float()\n",
    "  stop_token_padded[:, -1] = 1.0\n",
    "  \n",
    "  return texts_padded, \\\n",
    "         text_lengths, \\\n",
    "         mels_padded, \\\n",
    "         mel_lengths, \\\n",
    "         stop_token_padded "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Dataset and DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(hp.csv_path)\n",
    "dataset = TextMelDataset(df)\n",
    "train_loader = DataLoader(\n",
    "    dataset, \n",
    "    num_workers=2, \n",
    "    shuffle=True,\n",
    "    batch_size=hp.batch_size,\n",
    "    pin_memory=True, \n",
    "    drop_last=True, \n",
    "    collate_fn=text_mel_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def names_shape(names, shape):  \n",
    "    return \"(\" + \", \".join([f\"{k}={v}\" for k, v in list(zip(names, shape))]) + \")\"\n",
    "\n",
    "for i, batch in enumerate(train_loader):\n",
    "    text_padded, text_lengths, mel_padded, mel_lengths, stop_token_padded = batch\n",
    "    print(f\"=========batch {i}=========\")\n",
    "    print(\"text_padded:\", names_shape([\"N\", \"S\"], text_padded.shape))\n",
    "    print(\"text_lengths:\", names_shape([\"N\"], text_lengths.shape))\n",
    "    print(\"mel_padded:\", names_shape([\"N\", \"TIME\", \"FREQ\"], mel_padded.shape))\n",
    "    print(\"mel_lengths:\", names_shape([\"N\"], mel_lengths.shape))\n",
    "    print(\"stop_token_padded:\", names_shape([\"N\", \"TIME\"], stop_token_padded.shape))\n",
    "    if i > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TTSLoss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TTSLoss, self).__init__()\n",
    "        self.mse_loss = torch.nn.MSELoss()\n",
    "        self.bce_loss = torch.nn.BCEWithLogitsLoss()\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        mel_postnet_out,\n",
    "        mel_out,\n",
    "        stop_token_out,\n",
    "        mel_target,\n",
    "        stop_token_target\n",
    "    ):\n",
    "        stop_token_target = stop_token_target.view(-1,1)\n",
    "        stop_token_out = stop_token_out.view(-1,1)\n",
    "        mel_loss = self.mse_loss(mel_out, mel_target) + \\\n",
    "            self.mse_loss(mel_postnet_out, mel_target)\n",
    "            \n",
    "        stop_token_loss = self.bce_loss(stop_token_out, stop_token_target) * hp.r_gate\n",
    "        \n",
    "        return mel_loss + stop_token_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_mp3(x, f=\"audio.mp3\", sr=hp.sr, normalized=True):\n",
    "    pydub.AudioSegment(\n",
    "        x.detach().cpu().numpy(),\n",
    "        frame_rate=sr,\n",
    "        sample_width=2, \n",
    "        channels=1\n",
    "    ).export(f, format=\"mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EncoderBlock, self).__init__()\n",
    "    self.norm_1 = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "    self.attn = torch.nn.MultiheadAttention(\n",
    "      embed_dim=hp.embedding_size,\n",
    "      num_heads=4,\n",
    "      dropout=0.1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    self.norm_2 = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "\n",
    "    self.linear_1 = nn.Linear(\n",
    "      hp.embedding_size, \n",
    "      hp.dim_feedforward\n",
    "    )\n",
    "\n",
    "    self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "    self.linear_2 = nn.Linear(\n",
    "      hp.dim_feedforward, \n",
    "      hp.embedding_size\n",
    "    )\n",
    "    self.dropout_3 = torch.nn.Dropout(0.1)\n",
    "    \n",
    "\n",
    "  def forward(\n",
    "    self, \n",
    "    x,\n",
    "    attn_mask = None, \n",
    "    key_padding_mask = None\n",
    "  ):\n",
    "    x_out = self.norm_1(x)\n",
    "    x_out, _ = self.attn(\n",
    "      query=x_out, \n",
    "      key=x_out, \n",
    "      value=x_out,\n",
    "      attn_mask=attn_mask,\n",
    "      key_padding_mask=key_padding_mask\n",
    "    )\n",
    "    x_out = self.dropout_1(x_out)\n",
    "    x = x + x_out    \n",
    "\n",
    "    x_out = self.norm_2(x) \n",
    "\n",
    "    x_out = self.linear_1(x_out)\n",
    "    x_out = F.relu(x_out)\n",
    "    x_out = self.dropout_2(x_out)\n",
    "    x_out = self.linear_2(x_out)\n",
    "    x_out = self.dropout_3(x_out)\n",
    "\n",
    "    x = x + x_out\n",
    "    \n",
    "    return x\n",
    "\n",
    "\n",
    "class DecoderBlock(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DecoderBlock, self).__init__()\n",
    "    self.norm_1 = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "    self.self_attn = torch.nn.MultiheadAttention(\n",
    "      embed_dim=hp.embedding_size,\n",
    "      num_heads=4,\n",
    "      dropout=0.1,\n",
    "      batch_first=True\n",
    "    )\n",
    "    self.dropout_1 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    self.norm_2 = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "    self.attn = torch.nn.MultiheadAttention(\n",
    "      embed_dim=hp.embedding_size,\n",
    "      num_heads=4,\n",
    "      dropout=0.1,\n",
    "      batch_first=True\n",
    "    )    \n",
    "    self.dropout_2 = torch.nn.Dropout(0.1)\n",
    "\n",
    "    self.norm_3 = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "\n",
    "    self.linear_1 = nn.Linear(\n",
    "      hp.embedding_size, \n",
    "      hp.dim_feedforward\n",
    "    )\n",
    "    self.dropout_3 = torch.nn.Dropout(0.1)\n",
    "    self.linear_2 = nn.Linear(\n",
    "      hp.dim_feedforward, \n",
    "      hp.embedding_size\n",
    "    )\n",
    "    self.dropout_4 = torch.nn.Dropout(0.1)\n",
    "\n",
    "\n",
    "  def forward(\n",
    "    self,     \n",
    "    x,\n",
    "    memory,\n",
    "    x_attn_mask = None, \n",
    "    x_key_padding_mask = None,\n",
    "    memory_attn_mask = None,\n",
    "    memory_key_padding_mask = None\n",
    "  ):\n",
    "    x_out, _ = self.self_attn(\n",
    "      query=x, \n",
    "      key=x, \n",
    "      value=x,\n",
    "      attn_mask=x_attn_mask,\n",
    "      key_padding_mask=x_key_padding_mask\n",
    "    )\n",
    "    x_out = self.dropout_1(x_out)\n",
    "    x = self.norm_1(x + x_out)\n",
    "     \n",
    "    x_out, _ = self.attn(\n",
    "      query=x,\n",
    "      key=memory,\n",
    "      value=memory,\n",
    "      attn_mask=memory_attn_mask,\n",
    "      key_padding_mask=memory_key_padding_mask\n",
    "    )\n",
    "    x_out = self.dropout_2(x_out)\n",
    "    x = self.norm_2(x + x_out)\n",
    "\n",
    "    x_out = self.linear_1(x)\n",
    "    x_out = F.relu(x_out)\n",
    "    x_out = self.dropout_3(x_out)\n",
    "    x_out = self.linear_2(x_out)\n",
    "    x_out = self.dropout_4(x_out)\n",
    "    x = self.norm_3(x + x_out)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class EncoderPreNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(EncoderPreNet, self).__init__()\n",
    "    \n",
    "    self.embedding = nn.Embedding(\n",
    "        num_embeddings=hp.text_num_embeddings,\n",
    "        embedding_dim=hp.encoder_embedding_size\n",
    "    )\n",
    "\n",
    "    self.linear_1 = nn.Linear(\n",
    "      hp.encoder_embedding_size, \n",
    "      hp.encoder_embedding_size\n",
    "    )\n",
    "\n",
    "    self.linear_2 = nn.Linear(\n",
    "      hp.encoder_embedding_size, \n",
    "      hp.embedding_size\n",
    "    )\n",
    "\n",
    "    self.conv_1 = nn.Conv1d(\n",
    "      hp.encoder_embedding_size, \n",
    "      hp.encoder_embedding_size,\n",
    "      kernel_size=hp.encoder_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.encoder_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_1 = nn.BatchNorm1d(\n",
    "      hp.encoder_embedding_size\n",
    "    )\n",
    "    self.dropout_1 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    self.conv_2 = nn.Conv1d(\n",
    "      hp.encoder_embedding_size, \n",
    "      hp.encoder_embedding_size,\n",
    "      kernel_size=hp.encoder_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.encoder_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_2 = nn.BatchNorm1d(\n",
    "      hp.encoder_embedding_size\n",
    "    )\n",
    "    self.dropout_2 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    self.conv_3 = nn.Conv1d(\n",
    "      hp.encoder_embedding_size, \n",
    "      hp.encoder_embedding_size,\n",
    "      kernel_size=hp.encoder_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.encoder_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_3 = nn.BatchNorm1d(\n",
    "      hp.encoder_embedding_size\n",
    "    )\n",
    "    self.dropout_3 = torch.nn.Dropout(0.5)    \n",
    "\n",
    "  def forward(self, text):\n",
    "    x = self.embedding(text) # (N, S, E)\n",
    "    x = self.linear_1(x)\n",
    "\n",
    "    x = x.transpose(2, 1) # (N, E, S) \n",
    "\n",
    "    x = self.conv_1(x)\n",
    "    x = self.bn_1(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout_1(x)\n",
    "\n",
    "    x = self.conv_2(x)\n",
    "    x = self.bn_2(x)\n",
    "    x = F.relu(x)\n",
    "    x = self.dropout_2(x)\n",
    "    \n",
    "    x = self.conv_3(x)\n",
    "    x = self.bn_3(x)    \n",
    "    x = F.relu(x)\n",
    "    x = self.dropout_3(x)\n",
    "\n",
    "    x = x.transpose(1, 2) # (N, S, E)\n",
    "    x = self.linear_2(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class PostNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(PostNet, self).__init__()  \n",
    "    \n",
    "    self.conv_1 = nn.Conv1d(\n",
    "      hp.mel_freq, \n",
    "      hp.postnet_embedding_size,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_1 = nn.BatchNorm1d(\n",
    "      hp.postnet_embedding_size\n",
    "    )\n",
    "    self.dropout_1 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    self.conv_2 = nn.Conv1d(\n",
    "      hp.postnet_embedding_size, \n",
    "      hp.postnet_embedding_size,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_2 = nn.BatchNorm1d(\n",
    "      hp.postnet_embedding_size\n",
    "    )\n",
    "    self.dropout_2 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    self.conv_3 = nn.Conv1d(\n",
    "      hp.postnet_embedding_size, \n",
    "      hp.postnet_embedding_size,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_3 = nn.BatchNorm1d(\n",
    "      hp.postnet_embedding_size\n",
    "    )\n",
    "    self.dropout_3 = torch.nn.Dropout(0.5)\n",
    "\n",
    "    self.conv_4 = nn.Conv1d(\n",
    "      hp.postnet_embedding_size, \n",
    "      hp.postnet_embedding_size,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_4 = nn.BatchNorm1d(\n",
    "      hp.postnet_embedding_size\n",
    "    )\n",
    "    self.dropout_4 = torch.nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    self.conv_5 = nn.Conv1d(\n",
    "      hp.postnet_embedding_size, \n",
    "      hp.postnet_embedding_size,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_5 = nn.BatchNorm1d(\n",
    "      hp.postnet_embedding_size\n",
    "    )\n",
    "    self.dropout_5 = torch.nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "    self.conv_6 = nn.Conv1d(\n",
    "      hp.postnet_embedding_size, \n",
    "      hp.mel_freq,\n",
    "      kernel_size=hp.postnet_kernel_size, \n",
    "      stride=1,\n",
    "      padding=int((hp.postnet_kernel_size - 1) / 2), \n",
    "      dilation=1\n",
    "    )\n",
    "    self.bn_6 = nn.BatchNorm1d(hp.mel_freq)\n",
    "    self.dropout_6 = torch.nn.Dropout(0.5)\n",
    "\n",
    "\n",
    "  def forward(self, x):\n",
    "    # x - (N, TIME, FREQ)\n",
    "\n",
    "    x = x.transpose(2, 1) # (N, FREQ, TIME)\n",
    "\n",
    "    x = self.conv_1(x)\n",
    "    x = self.bn_1(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout_1(x) # (N, POSNET_DIM, TIME)\n",
    "\n",
    "    x = self.conv_2(x)\n",
    "    x = self.bn_2(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout_2(x) # (N, POSNET_DIM, TIME)\n",
    "\n",
    "    x = self.conv_3(x)\n",
    "    x = self.bn_3(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout_3(x) # (N, POSNET_DIM, TIME)    \n",
    "\n",
    "    x = self.conv_4(x)\n",
    "    x = self.bn_4(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout_4(x) # (N, POSNET_DIM, TIME)    \n",
    "\n",
    "    x = self.conv_5(x)\n",
    "    x = self.bn_5(x)\n",
    "    x = torch.tanh(x)\n",
    "    x = self.dropout_5(x) # (N, POSNET_DIM, TIME)\n",
    "\n",
    "    x = self.conv_6(x)\n",
    "    x = self.bn_6(x)\n",
    "    x = self.dropout_6(x) # (N, FREQ, TIME)\n",
    "\n",
    "    x = x.transpose(1, 2)\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "class DecoderPreNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(DecoderPreNet, self).__init__()\n",
    "    self.linear_1 = nn.Linear(\n",
    "      hp.mel_freq, \n",
    "      hp.embedding_size\n",
    "    )\n",
    "\n",
    "    self.linear_2 = nn.Linear(\n",
    "      hp.embedding_size, \n",
    "      hp.embedding_size\n",
    "    )\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = self.linear_1(x)\n",
    "    x = F.relu(x)\n",
    "    \n",
    "    x = F.dropout(x, p=0.5, training=True)\n",
    "\n",
    "    x = self.linear_2(x)\n",
    "    x = F.relu(x)    \n",
    "    x = F.dropout(x, p=0.5, training=True)\n",
    "\n",
    "    return x    \n",
    "\n",
    "\n",
    "class TransformerTTS(nn.Module):\n",
    "  def __init__(self, device=\"cuda\"):\n",
    "    super(TransformerTTS, self).__init__()\n",
    "\n",
    "    self.encoder_prenet = EncoderPreNet()\n",
    "    self.decoder_prenet = DecoderPreNet()\n",
    "    self.postnet = PostNet()\n",
    "\n",
    "    self.pos_encoding = nn.Embedding(\n",
    "        num_embeddings=hp.max_mel_time, \n",
    "        embedding_dim=hp.embedding_size\n",
    "    )\n",
    "\n",
    "    self.encoder_block_1 = EncoderBlock()\n",
    "    self.encoder_block_2 = EncoderBlock()\n",
    "    self.encoder_block_3 = EncoderBlock()\n",
    "\n",
    "    self.decoder_block_1 = DecoderBlock()\n",
    "    self.decoder_block_2 = DecoderBlock()\n",
    "    self.decoder_block_3 = DecoderBlock()\n",
    "\n",
    "    self.linear_1 = nn.Linear(hp.embedding_size, hp.mel_freq) \n",
    "    self.linear_2 = nn.Linear(hp.embedding_size, 1)\n",
    "\n",
    "    self.norm_memory = nn.LayerNorm(\n",
    "      normalized_shape=hp.embedding_size\n",
    "    )\n",
    "\n",
    "\n",
    "  def forward(\n",
    "    self, \n",
    "    text, \n",
    "    text_len,\n",
    "    mel, \n",
    "    mel_len\n",
    "  ):  \n",
    "    \n",
    "    N = text.shape[0]\n",
    "    S = text.shape[1]\n",
    "    TIME = mel.shape[1]\n",
    "\n",
    "    self.src_key_padding_mask = torch.zeros(\n",
    "        (N, S),\n",
    "        device=text.device\n",
    "    ).masked_fill(\n",
    "      ~mask_from_seq_lengths(\n",
    "        text_len,\n",
    "        max_length=S\n",
    "      ),\n",
    "      float(\"-inf\")\n",
    "    )\n",
    "    \n",
    "    self.src_mask = torch.zeros(\n",
    "      (S, S),\n",
    "      device=text.device\n",
    "    ).masked_fill(\n",
    "      torch.triu(\n",
    "          torch.full(\n",
    "              (S, S), \n",
    "              True,\n",
    "              dtype=torch.bool\n",
    "          ), \n",
    "          diagonal=1\n",
    "      ).to(text.device),       \n",
    "      float(\"-inf\")\n",
    "    )\n",
    "\n",
    "    self.tgt_key_padding_mask = torch.zeros(\n",
    "      (N, TIME),\n",
    "      device=mel.device\n",
    "    ).masked_fill(\n",
    "      ~mask_from_seq_lengths(\n",
    "        mel_len,\n",
    "        max_length=TIME\n",
    "      ),\n",
    "      float(\"-inf\")\n",
    "    )\n",
    "\n",
    "    self.tgt_mask = torch.zeros(\n",
    "      (TIME, TIME),\n",
    "      device=mel.device\n",
    "    ).masked_fill(\n",
    "      torch.triu(\n",
    "          torch.full(\n",
    "              (TIME, TIME), \n",
    "              True,\n",
    "              device=mel.device,\n",
    "              dtype=torch.bool\n",
    "          ), \n",
    "          diagonal=1\n",
    "      ),       \n",
    "      float(\"-inf\")\n",
    "    )\n",
    "\n",
    "    self.memory_mask = torch.zeros(\n",
    "      (TIME, S),\n",
    "      device=mel.device\n",
    "    ).masked_fill(\n",
    "      torch.triu(\n",
    "          torch.full(\n",
    "              (TIME, S), \n",
    "              True,\n",
    "              device=mel.device,\n",
    "              dtype=torch.bool\n",
    "          ), \n",
    "          diagonal=1          \n",
    "      ),       \n",
    "      float(\"-inf\")\n",
    "    )    \n",
    "\n",
    "    text_x = self.encoder_prenet(text) # (N, S, E)    \n",
    "    \n",
    "    pos_codes = self.pos_encoding(\n",
    "      torch.arange(hp.max_mel_time).to(mel.device)\n",
    "    ) # (MAX_S_TIME, E)\n",
    "\n",
    "    S = text_x.shape[1]\n",
    "    text_x = text_x + pos_codes[:S]\n",
    "    # dropout after pos encoding?\n",
    "\n",
    "    text_x = self.encoder_block_1(\n",
    "      text_x, \n",
    "      attn_mask = self.src_mask, \n",
    "      key_padding_mask = self.src_key_padding_mask\n",
    "    )\n",
    "    text_x = self.encoder_block_2(\n",
    "      text_x, \n",
    "      attn_mask = self.src_mask, \n",
    "      key_padding_mask = self.src_key_padding_mask\n",
    "    )    \n",
    "    text_x = self.encoder_block_3(\n",
    "      text_x, \n",
    "      attn_mask = self.src_mask, \n",
    "      key_padding_mask = self.src_key_padding_mask\n",
    "    ) # (N, S, E)\n",
    "\n",
    "    text_x = self.norm_memory(text_x)\n",
    "        \n",
    "    mel_x = self.decoder_prenet(mel) # (N, TIME, E)    \n",
    "    mel_x = mel_x + pos_codes[:TIME]\n",
    "    # dropout after pos encoding?\n",
    "\n",
    "    mel_x = self.decoder_block_1(\n",
    "      x=mel_x,\n",
    "      memory=text_x,\n",
    "      x_attn_mask=self.tgt_mask, \n",
    "      x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "      memory_attn_mask=self.memory_mask,\n",
    "      memory_key_padding_mask=self.src_key_padding_mask\n",
    "    )\n",
    "\n",
    "    mel_x = self.decoder_block_2(\n",
    "      x=mel_x,\n",
    "      memory=text_x,\n",
    "      x_attn_mask=self.tgt_mask, \n",
    "      x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "      memory_attn_mask=self.memory_mask,\n",
    "      memory_key_padding_mask=self.src_key_padding_mask\n",
    "    )\n",
    "\n",
    "    mel_x = self.decoder_block_3(\n",
    "      x=mel_x,\n",
    "      memory=text_x,\n",
    "      x_attn_mask=self.tgt_mask, \n",
    "      x_key_padding_mask=self.tgt_key_padding_mask,\n",
    "      memory_attn_mask=self.memory_mask,\n",
    "      memory_key_padding_mask=self.src_key_padding_mask\n",
    "    ) # (N, TIME, E)\n",
    "\n",
    "    mel_linear = self.linear_1(mel_x) # (N, TIME, FREQ)\n",
    "    mel_postnet = self.postnet(mel_linear) # (N, TIME, FREQ)\n",
    "    mel_postnet = mel_linear + mel_postnet # (N, TIME, FREQ)\n",
    "    stop_token = self.linear_2(mel_x) # (N, TIME, 1)\n",
    "\n",
    "    bool_mel_mask = self.tgt_key_padding_mask.ne(0).unsqueeze(-1).repeat(\n",
    "      1, 1, hp.mel_freq\n",
    "    )\n",
    "\n",
    "    mel_linear = mel_linear.masked_fill(\n",
    "      bool_mel_mask,\n",
    "      0\n",
    "    )\n",
    "\n",
    "    mel_postnet = mel_postnet.masked_fill(\n",
    "      bool_mel_mask,\n",
    "      0      \n",
    "    )\n",
    "\n",
    "    stop_token = stop_token.masked_fill(\n",
    "      bool_mel_mask[:, :, 0].unsqueeze(-1),\n",
    "      1e3\n",
    "    ).squeeze(2)\n",
    "    \n",
    "    return mel_postnet, mel_linear, stop_token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "train_dataset = TextMelDataset(train_df)\n",
    "val_dataset = TextMelDataset(val_df)\n",
    "train_loader = DataLoader(train_dataset, batch_size=hp.batch_size, shuffle=True, collate_fn=text_mel_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=hp.batch_size, shuffle=False, collate_fn=text_mel_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x1fed90df050>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = TransformerTTS().cuda()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = TTSLoss().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerTTS(\n",
      "  (encoder_prenet): EncoderPreNet(\n",
      "    (embedding): Embedding(86, 512)\n",
      "    (linear_1): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (linear_2): Linear(in_features=512, out_features=256, bias=True)\n",
      "    (conv_1): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn_1): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_1): Dropout(p=0.5, inplace=False)\n",
      "    (conv_2): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn_2): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "    (conv_3): Conv1d(512, 512, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "    (bn_3): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_3): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (decoder_prenet): DecoderPreNet(\n",
      "    (linear_1): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (linear_2): Linear(in_features=256, out_features=256, bias=True)\n",
      "  )\n",
      "  (postnet): PostNet(\n",
      "    (conv_1): Conv1d(128, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_1): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_1): Dropout(p=0.5, inplace=False)\n",
      "    (conv_2): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_2): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_2): Dropout(p=0.5, inplace=False)\n",
      "    (conv_3): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_3): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_3): Dropout(p=0.5, inplace=False)\n",
      "    (conv_4): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_4): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_4): Dropout(p=0.5, inplace=False)\n",
      "    (conv_5): Conv1d(1024, 1024, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_5): BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_5): Dropout(p=0.5, inplace=False)\n",
      "    (conv_6): Conv1d(1024, 128, kernel_size=(5,), stride=(1,), padding=(2,))\n",
      "    (bn_6): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (dropout_6): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (pos_encoding): Embedding(1024, 256)\n",
      "  (encoder_block_1): EncoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_block_2): EncoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (encoder_block_3): EncoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder_block_1): DecoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_4): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder_block_2): DecoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_4): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (decoder_block_3): DecoderBlock(\n",
      "    (norm_1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (self_attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_1): Dropout(p=0.1, inplace=False)\n",
      "    (norm_2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (attn): MultiheadAttention(\n",
      "      (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "    )\n",
      "    (dropout_2): Dropout(p=0.1, inplace=False)\n",
      "    (norm_3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "    (linear_1): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    (dropout_3): Dropout(p=0.1, inplace=False)\n",
      "    (linear_2): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    (dropout_4): Dropout(p=0.1, inplace=False)\n",
      "  )\n",
      "  (linear_1): Linear(in_features=256, out_features=128, bias=True)\n",
      "  (linear_2): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (norm_memory): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "train_saved_path: params/train_SimpleTransfromerTTS.pt\n",
      "test_saved_path: params/test_SimpleTransfromerTTS.pt\n",
      "Start from zero!\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def batch_process(batch):\n",
    "  text_padded, \\\n",
    "  text_lengths, \\\n",
    "  mel_padded, \\\n",
    "  mel_lengths, \\\n",
    "  stop_token_padded = batch\n",
    "\n",
    "  text_padded = text_padded.cuda()\n",
    "  text_lengths = text_lengths.cuda()\n",
    "  mel_padded = mel_padded.cuda()\n",
    "  stop_token_padded = stop_token_padded.cuda()\n",
    "  mel_lengths = mel_lengths.cuda()\n",
    "\n",
    "  N = mel_padded.shape[0]\n",
    "  SOS = torch.zeros((N, 1, hp.mel_freq), device=mel_padded.device) # Start of sequence\n",
    "  \n",
    "  mel_input = torch.cat(\n",
    "    [\n",
    "      SOS, \n",
    "      mel_padded[:, :-1, :] # (N, L, FREQ)\n",
    "    ],\n",
    "    dim=1\n",
    "  )  \n",
    "\n",
    "  return text_padded, \\\n",
    "         text_lengths, \\\n",
    "         mel_padded, \\\n",
    "         mel_lengths, \\\n",
    "         mel_input, \\\n",
    "         stop_token_padded\n",
    "\n",
    "\n",
    "\n",
    "def inference_utterance(model, text):\n",
    "  sequences = text_to_seq(text).unsqueeze(0).cuda()\n",
    "  postnet_mel, stop_token = model.inference(\n",
    "    sequences, \n",
    "    stop_token_threshold=1e5, \n",
    "    with_tqdm = False\n",
    "  )          \n",
    "  audio = inverse_mel_spec_to_wav(postnet_mel.detach()[0].T)\n",
    "            \n",
    "  fig, (ax1) = plt.subplots(1, 1)\n",
    "  ax1.imshow(\n",
    "      postnet_mel[0, :, :].detach().cpu().numpy().T, \n",
    "  )\n",
    "  \n",
    "  return audio, fig \n",
    "\n",
    "\n",
    "def calculate_test_loss(model, test_loader):\n",
    "  test_loss_mean = 0.0\n",
    "  model.eval()\n",
    "\n",
    "  with torch.no_grad():\n",
    "    for test_i, test_batch in enumerate(test_loader):\n",
    "      test_text_padded, \\\n",
    "      test_text_lengths, \\\n",
    "      test_mel_padded, \\\n",
    "      test_mel_lengths, \\\n",
    "      test_mel_input, \\\n",
    "      test_stop_token_padded = batch_process(batch)\n",
    "\n",
    "      test_post_mel_out, test_mel_out, test_stop_token_out = model(\n",
    "        test_text_padded, \n",
    "        test_text_lengths,\n",
    "        test_mel_input, \n",
    "        test_mel_lengths\n",
    "      )        \n",
    "      test_loss = criterion(\n",
    "        mel_postnet_out = test_post_mel_out, \n",
    "        mel_out = test_mel_out, \n",
    "        stop_token_out = test_stop_token_out, \n",
    "        mel_target = test_mel_padded, \n",
    "        stop_token_target = test_stop_token_padded\n",
    "      )\n",
    "\n",
    "      test_loss_mean += test_loss.item()\n",
    "\n",
    "  test_loss_mean = test_loss_mean / (test_i + 1)  \n",
    "  return test_loss_mean\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "  torch.manual_seed(hp.seed)\n",
    "\n",
    "  df = pd.read_csv(hp.csv_path)  \n",
    "  train_df, test_df = train_test_split(\n",
    "    df, \n",
    "    test_size=64, \n",
    "    random_state=hp.seed\n",
    "  )\n",
    "  train_loader = torch.utils.data.DataLoader(\n",
    "      TextMelDataset(train_df), \n",
    "      num_workers=2, \n",
    "      shuffle=True,\n",
    "      sampler=None, \n",
    "      batch_size=hp.batch_size,\n",
    "      pin_memory=True, \n",
    "      drop_last=True, \n",
    "      collate_fn=text_mel_collate_fn\n",
    "  )\n",
    "  test_loader = torch.utils.data.DataLoader(\n",
    "      TextMelDataset(test_df), \n",
    "      num_workers=2, \n",
    "      shuffle=True,\n",
    "      sampler=None, \n",
    "      batch_size=8,\n",
    "      pin_memory=True, \n",
    "      drop_last=True, \n",
    "      collate_fn=text_mel_collate_fn\n",
    "  )  \n",
    "  \n",
    "  train_saved_path = f\"{hp.save_path}/train_{hp.save_name}\"\n",
    "  test_saved_path = f\"{hp.save_path}/test_{hp.save_name}\"\n",
    "  \n",
    "  print(\"train_saved_path:\", train_saved_path)\n",
    "  print(\"test_saved_path:\", test_saved_path)\n",
    "\n",
    "  logger = SummaryWriter(hp.log_path)  \n",
    "  criterion = TTSLoss().cuda()\n",
    "  model = TransformerTTS().cuda()\n",
    "  optimizer = torch.optim.AdamW(model.parameters(), lr=hp.lr)\n",
    "  scaler = torch.cuda.amp.GradScaler()  \n",
    "\n",
    "  best_test_loss_mean = float(\"inf\")\n",
    "  best_train_loss_mean = float(\"inf\")\n",
    "  \n",
    "  train_loss_mean = 0.0\n",
    "  epoch = 0\n",
    "  i = 0\n",
    "\n",
    "  if os.path.isfile(train_saved_path):  \n",
    "    state = torch.load(train_saved_path)\n",
    "    state_model = state[\"model\"]\n",
    "    state_optimizer = state[\"optimizer\"]\n",
    "    \n",
    "    i = state[\"i\"] + 1\n",
    "    best_test_loss_mean = state.get(\"test_loss\", float(\"inf\"))\n",
    "    best_train_loss_mean = state.get(\"train_loss\", float(\"inf\"))\n",
    "\n",
    "    model.load_state_dict(state_model)\n",
    "    optimizer.load_state_dict(state_optimizer)\n",
    "\n",
    "    print(f\"Load: {i}; test_loss: {np.round(best_test_loss_mean, 5)}; train_loss: {np.round(best_train_loss_mean, 5)}\")\n",
    "  else:\n",
    "    print(\"Start from zero!\")\n",
    "\n",
    "\n",
    "  start_time_sec = time.time()\n",
    "  while True:\n",
    "    for batch in train_loader:      \n",
    "      text_padded, \\\n",
    "      text_lengths, \\\n",
    "      mel_padded, \\\n",
    "      mel_lengths, \\\n",
    "      mel_input, \\\n",
    "      stop_token_padded = batch_process(batch)\n",
    "\n",
    "      model.train(True)\n",
    "      model.zero_grad()\n",
    "\n",
    "      with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        post_mel_out, mel_out, stop_token_out = model(\n",
    "          text_padded, \n",
    "          text_lengths,\n",
    "          mel_input, \n",
    "          mel_lengths\n",
    "        )        \n",
    "        loss = criterion(\n",
    "          mel_postnet_out = post_mel_out, \n",
    "          mel_out = mel_out, \n",
    "          stop_token_out = stop_token_out, \n",
    "          mel_target = mel_padded, \n",
    "          stop_token_target = stop_token_padded\n",
    "        )\n",
    "\n",
    "      scaler.scale(loss).backward()      \n",
    "      scaler.unscale_(optimizer)\n",
    "      torch.nn.utils.clip_grad_norm_(model.parameters(), hp.grad_clip)\n",
    "      scaler.step(optimizer)\n",
    "      scaler.update()\n",
    "\n",
    "      train_loss_mean += loss.item()      \n",
    "\n",
    "      if i !=0 and i % hp.step_print == 0:\n",
    "        train_loss_mean = train_loss_mean / hp.step_print        \n",
    "        logger.add_scalar(\"Loss/train_loss\", train_loss_mean, global_step=i)\n",
    "        \n",
    "        if i % hp.step_test == 0:            \n",
    "          test_loss_mean = calculate_test_loss(model, test_loader)\n",
    "          audio, fig = inference_utterance(model, \"Hello, World.\")\n",
    "\n",
    "          logger.add_scalar(\"Loss/test_loss\", test_loss_mean, global_step=i)\n",
    "          logger.add_figure(f\"Img/img_{i}\", fig, global_step=i) \n",
    "          logger.add_audio(f\"Utterance/audio_{i}\",audio, sample_rate=hp.sr, global_step=i)\n",
    "          \n",
    "          print(f\"{epoch}-{i}) Test loss: {np.round(test_loss_mean, 5)}\")\n",
    "\n",
    "          if i % hp.step_save == 0:\n",
    "            is_best_train = train_loss_mean < best_train_loss_mean\n",
    "            is_best_test = test_loss_mean < best_test_loss_mean\n",
    "\n",
    "            state = {\n",
    "              \"model\": model.state_dict(),\n",
    "              \"optimizer\": optimizer.state_dict(),\n",
    "              \"i\": i,\n",
    "              \"test_loss\": test_loss_mean,\n",
    "              \"train_loss\": train_loss_mean\n",
    "            }\n",
    "\n",
    "            if is_best_train:\n",
    "              print(f\"{epoch}-{i}) Save best train\")\n",
    "              torch.save(state, train_saved_path)\n",
    "              best_train_loss_mean = train_loss_mean\n",
    "\n",
    "            if is_best_test:\n",
    "              print(f\"{epoch}-{i}) Save best test\")\n",
    "              torch.save(state, test_saved_path)\n",
    "              best_test_loss_mean = test_loss_mean\n",
    "              \n",
    "\n",
    "        end_time_sec = time.time()\n",
    "        time_sec = np.round(end_time_sec - start_time_sec, 3)\n",
    "        start_time_sec = end_time_sec\n",
    "        \n",
    "        print(f\"{epoch}-{i}) Train loss: {np.round(train_loss_mean, 5)}; Duration: {time_sec} sec.\")\n",
    "        train_loss_mean = 0.0\n",
    "\n",
    "      i += 1\n",
    "    epoch += 1   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
